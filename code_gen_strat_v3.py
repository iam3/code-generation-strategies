# -*- coding: utf-8 -*-
"""code_gen_strat_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zqGDDmisMCZ5P4xuQqdkvvhg1a78kDXN
"""

"""
Improved strategyâ€‘selector training script
====================================================
â€¢ Simplifies label taxonomy (flat multiâ€‘label)
â€¢ Vectorises data processing with ðŸ¤— Datasets â†’ faster & clearer
â€¢ Uses SBERT embeddings cached via map() & multiprocessing
â€¢ Adds classâ€‘weighted LogisticRegression + FocalLoss option
â€¢ Saves artefacts + label mapping for downstream use

Author: ChatGPT (improved for user)
"""

# -----------------------------------------------------
# 0. Install deps (Colabâ€‘safe; comment out if local)
# -----------------------------------------------------
try:
    import datasets, sentence_transformers, sklearn  # noqa: F401
except ImportError:
    import subprocess, sys
    subprocess.run([sys.executable, "-m", "pip", "-q", "install",
                    "datasets", "sentence-transformers", "scikit-learn", "tqdm", "joblib"])

# -----------------------------------------------------
# 1. Imports & Config
# -----------------------------------------------------
import re, random, json, joblib, itertools
from collections import Counter
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datasets import load_dataset, Dataset, DatasetDict
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from sklearn.preprocessing import MultiLabelBinarizer
from tqdm.auto import tqdm

SEED = 42
random.seed(SEED); np.random.seed(SEED)

# Save paths
ARTIFACT_DIR = Path("artifacts"); ARTIFACT_DIR.mkdir(exist_ok=True)

# -----------------------------------------------------
# 2. Dataset Helpers
# -----------------------------------------------------

HF_DESIGN_PROMPTS = "ireneom3/design-pattern-prompts"


def to_prompts(ds, field: str, tag: str):
    """Return list of {id, prompt} dicts with basic filtering."""
    return [{"id": f"{tag}_{i}", "prompt": ex[field].strip()}
            for i, ex in enumerate(ds)
            if ex.get(field) and len(ex[field].strip()) >= 8]


# -----------------------------------------------------
# 3. Load raw datasets
# -----------------------------------------------------
print("ðŸ”¹ Loading raw splits â€¦")
ct     = load_dataset("code_x_glue_ct_code_to_text", "python", split="train")
tc     = load_dataset("code_x_glue_tc_text_to_code",          split="train")
refine = load_dataset("code_x_glue_cc_code_refinement", "medium", split="train")
dp     = load_dataset(HF_DESIGN_PROMPTS, split="train")

raw_prompts = (
      to_prompts(ct, "docstring", "ct")
    + to_prompts(tc, "nl",        "tc")
    + [{"id": f"ref_{i}", "prompt": f"Fix the following buggy function:\n{ex['buggy'][:120]}â€¦"}
       for i, ex in enumerate(refine)]
    + to_prompts(dp, "prompt",    "dp")
)
train_data = Dataset.from_list(raw_prompts)
print(f"âœ… Collected {len(train_data):,} prompt examples\n")

# -----------------------------------------------------
# 4. Heuristic labelling + Merged taxonomy
# -----------------------------------------------------
PATTERN_RULES = {
    "Reflection Loop": r"\b(fix|debug|error|retry)\b",
    "Design Pattern" : r"\b(factory|observer|strategy|singleton|adapter|decorator|pattern)\b",
    "Planner-Coder"  : r"\b(first.*then|plan|steps?)\b",
    "Bug Fix"        : r"\b(bug|wrong output|defect)\b",
    "Refactor"       : r"\b(refactor|clean up|rename)\b",
    "Data Pipeline"  : r"\b(etl|csv|json|load.*file|pipeline)\b",
    "Documentation"  : r"\b(docstring|generate docs|readme)\b",
    "Complex Algorithm": r"\b(dynamic programming|graph|dijkstra|bfs|dfs)\b",
    "Integration Test": r"\b(integration test|e2e|harness)\b",
    "Direct Coding"  : r""  # fallback if nothing else hits
}
FALLBACK_RGX = re.compile(r"\b(class|plan|design)\b")


def heuristic_labels(text: str):
    """Return *merged* label list for a prompt."""
    low = text.lower()
    hits = [name for name, rgx in PATTERN_RULES.items() if rgx and re.search(rgx, low)]
    if not hits and (len(low.split()) < 12 and not FALLBACK_RGX.search(low)):
        hits = ["Direct Coding"]
    return hits or ["Direct Coding"]

print("ðŸ”¹ Tagging with heuristics â€¦")
train_data = train_data.map(lambda ex: {"labels": heuristic_labels(ex["prompt"])},
                            desc="heuristicâ€‘tagging")

# -----------------------------------------------------
# 5. Balancing (caps + boosts)
# -----------------------------------------------------
MAX_CAP    = 4_000
MIN_TARGET = 600
label_counter = Counter(itertools.chain.from_iterable(train_data["labels"]))

# collect indices to keep
keep_idx = []
for i, lbls in enumerate(train_data["labels"]):
    # oversample rare labels
    for lbl in lbls:
        if label_counter[lbl] < MIN_TARGET:
            keep_idx.append(i)  # replicate once now; will replicate more later if needed
    keep_idx.append(i)

balanced_ds = train_data.select(keep_idx).shuffle(seed=SEED)

print("Class counts after initial balance:")
print(Counter(itertools.chain.from_iterable(balanced_ds["labels"])) )

# -----------------------------------------------------
# 6. Sentence Embeddings (cached)
# -----------------------------------------------------
print("ðŸ”¹ Generating SBERT embeddings â€¦")
embedder = SentenceTransformer("all-MiniLM-L6-v2", device="cuda" if SentenceTransformer("all-MiniLM-L6-v2").device else "cpu")

# Avoid memory crash on Colab: encode all prompts outside .map()
prompts = balanced_ds["prompt"]
embeddings = embedder.encode(prompts, batch_size=128, show_progress_bar=True, convert_to_numpy=True)

# Add back to dataset safely
balanced_ds = balanced_ds.remove_columns("prompt")  # optionally remove to save space
balanced_ds = balanced_ds.add_column("emb", embeddings.tolist())
labels = balanced_ds["labels"]

# -----------------------------------------------------
# 7. Multiâ€‘label binarisation & split
# -----------------------------------------------------
mlb = MultiLabelBinarizer(); Y = mlb.fit_transform(labels)
X_train, X_test, y_train, y_test = train_test_split(
    embeddings, Y, test_size=0.25, random_state=SEED, stratify=Y)

# -----------------------------------------------------
# 8. Classifier (LogReg Oneâ€‘Vsâ€‘Rest with class weights)
# -----------------------------------------------------
print("ðŸ”¹ Training classifier â€¦")
# Compute class weights (inverse frequency)
class_freq = y_train.sum(axis=0)
weights = {i: max(class_freq)/f for i, f in enumerate(class_freq)}

clf = LogisticRegression(max_iter=3000, n_jobs=-1, class_weight=weights)
from sklearn.multiclass import OneVsRestClassifier
ovr = OneVsRestClassifier(clf).fit(X_train, y_train)

# -----------------------------------------------------
# 9. Evaluation
# -----------------------------------------------------
y_pred = ovr.predict(X_test)
print(classification_report(y_test, y_pred, target_names=mlb.classes_))

_, ax = plt.subplots(figsize=(10, 10))
ConfusionMatrixDisplay.from_estimator(ovr, X_test, y_test, ax=ax, cmap="Blues")
plt.title("Mergedâ€‘label Confusion Matrix")
plt.tight_layout(); plt.savefig(ARTIFACT_DIR/"confusion_matrix.png")

# -----------------------------------------------------
# 10. Save artefacts
# -----------------------------------------------------
print("ðŸ’¾ Saving artefacts â€¦")
mlb_path = ARTIFACT_DIR/"label_binarizer.pkl"
model_path = ARTIFACT_DIR/"ovr_logreg.pkl"
np.save(ARTIFACT_DIR/"sbert_embeddings.npy", embeddings)
joblib.dump(mlb, mlb_path)
joblib.dump(ovr, model_path)

print(f"âœ… Training complete. Model â†’ {model_path}\n   Label encoder â†’ {mlb_path}\n   Embeddings â†’ sbert_embeddings.npy\n   Confusion matrix figure saved.")