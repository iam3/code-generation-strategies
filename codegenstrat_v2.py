# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCHO1xF3QLgMWOGGgYTZHBGLy8VDWZkt
"""

# =============================================================
# 0. Install & login (run once in Colab)
# =============================================================
!pip -q install datasets sentence-transformers scikit-learn tqdm huggingface_hub
!pip install --upgrade datasets

from huggingface_hub import notebook_login
notebook_login()                        # paste your HF token

# =============================================================
# 1. Imports & config
# =============================================================
from datasets import load_dataset, Dataset
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
import numpy as np, pandas as pd, matplotlib.pyplot as plt, re, random, joblib
from tqdm.auto import tqdm
SEED = 42
random.seed(SEED); np.random.seed(SEED)

HF_DESIGN_PROMPTS = "ireneom3/design-pattern-prompts"

# =============================================================
# 2. Load & merge datasets
# =============================================================
ct     = load_dataset("code_x_glue_ct_code_to_text", "python", split="train")
tc     = load_dataset("code_x_glue_tc_text_to_code",          split="train")
refine = load_dataset("code_x_glue_cc_code_refinement", "medium", split="train")
dp     = load_dataset(HF_DESIGN_PROMPTS, split="train")

def to_prompts(ds, field, tag):
    return [
        {"id": f"{tag}_{i}", "prompt": ex[field].strip()}
        for i, ex in enumerate(ds)
        if ex.get(field) and len(ex[field].strip()) >= 8
    ]

prompts = (
      to_prompts(ct, "docstring", "ct")
    + to_prompts(tc, "nl",        "tc")
    + [{"id": f"ref_{i}", "prompt": f"Fix the following buggy function:\n{ex['buggy'][:120]}…"}
       for i, ex in enumerate(refine)]
    + to_prompts(dp, "prompt",    "dp")
)

train_data = Dataset.from_list(prompts)

# =============================================================
# 3. Heuristic labeling  (improved regex)
# =============================================================
pattern_rules = {
    "Reflection Loop": r"\b(fix|debug|error|retry)\b",
    "Design Pattern" : r"\b(factory|observer|strategy|singleton|adapter|decorator|pattern)\b",
    "Planner→Coder"  : r"\b(first.*then|plan|steps?)\b",
    "Bug Fix"        : r"\b(bug|wrong output|defect)\b",
    "Refactor"       : r"\b(refactor|clean up|rename)\b",
    "Data‑Pipeline"  : r"\b(etl|csv|json|load.*file|pipeline)\b",
    "Doc‑Generator"  : r"\b(docstring|generate docs|readme)\b",
    "Complex‑Algo"   : r"\b(dynamic programming|graph|dijkstra|bfs|dfs)\b",
    "Integration‑Test":r"\b(integration test|e2e|harness)\b"
}
fallback = re.compile(r"\b(class|plan|design)\b")

def heuristic(txt):
    p, hits = txt.lower(), set()
    if len(p.split()) < 12 and not fallback.search(p):
        return "Direct"
    for lab, rgx in pattern_rules.items():
        if re.search(rgx, p):
            hits.add(lab)
    if not hits: return "Direct"
    return ";".join(sorted(hits))

train_data = train_data.map(lambda ex: {"label": heuristic(ex["prompt"])})

# =============================================================
# 4. Hybrid balancing (undersample + oversample)
# =============================================================
MAX_CAP    = 5_000   # cut big classes
MIN_TARGET = 500     # boost small classes
from collections import defaultdict, Counter
buckets = defaultdict(list)
for idx, lbl in enumerate(train_data["label"]):
    buckets[lbl].append(idx)

balanced_idx = []
for lbl, idxs in buckets.items():
    if len(idxs) > MAX_CAP:
        balanced_idx += random.sample(idxs, MAX_CAP)
    else:
        balanced_idx += idxs

# oversample rare
counts = Counter([train_data["label"][i] for i in balanced_idx])
for lbl, cnt in counts.items():
    if cnt < MIN_TARGET:
        balanced_idx += random.choices(buckets[lbl], k=MIN_TARGET-cnt)

balanced_ds = train_data.select(balanced_idx).shuffle(seed=SEED)

# =============================================================
# 5. Embeddings & classifier
# =============================================================
embedder   = SentenceTransformer("all-MiniLM-L6-v2")
# Convert NumPy indices to Python integers before accessing the dataset
embeddings = embedder.encode([balanced_ds["prompt"][int(i)] for i in range(len(balanced_ds["prompt"]))], batch_size=64, show_progress_bar=True)

X_train, X_test, y_train, y_test = train_test_split(
    embeddings, balanced_ds["label"], stratify=balanced_ds["label"],
    test_size=0.25, random_state=SEED)

clf = LogisticRegression(max_iter=2000, multi_class="multinomial",
                         n_jobs=-1, random_state=SEED).fit(X_train, y_train)

print(classification_report(y_test, clf.predict(X_test)))

_, ax = plt.subplots(figsize=(10,10))
ConfusionMatrixDisplay.from_estimator(
    clf, X_test, y_test, ax=ax, cmap="Blues", xticks_rotation=45)
ax.set_xticklabels([t.get_text().replace(";", "\n") for t in ax.get_xticklabels()],
                   ha="right", fontsize=8)
ax.set_yticklabels([t.get_text().replace(";", "\n") for t in ax.get_yticklabels()],
                   fontsize=8)
plt.tight_layout(); plt.show()

# =============================================================
# 6. Save artifacts
# =============================================================
balanced_ds.save_to_disk("merged_strategy_dataset_balanced")
joblib.dump(clf, "strategy_selector_lr_balanced.joblib")
np.save("sbert_embeddings_balanced.npy", embeddings)